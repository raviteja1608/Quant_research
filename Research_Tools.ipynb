{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "Import the necessary libraries, such as requests, for making API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\ravit\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.50.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\ravit\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\ravit\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.30.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ravit\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ravit\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ravit\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ravit\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\ravit\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\ravit\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\ravit\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ravit\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ravit\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ravit\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.13.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ravit\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ravit\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ravit\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ravit\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ravit\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\ravit\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\ravit\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\ravit\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (4.13.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\ravit\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ravit\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ravit\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ravit\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (78.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\ravit\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ravit\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ravit\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary library for making API calls\n",
    "# Install necessary libraries\n",
    "'''%pip install requests\n",
    "%pip install pandas\n",
    "%pip install datetime\n",
    "%pip install cudf\n",
    "%pip install cupy\n",
    "%pip install openpyxl\n",
    "%pip install textblob\n",
    "%pip install transformers\n",
    "%pip install torch\n",
    "%pip install import-ipynb\n",
    "%pip install scikit-learn'''\n",
    "# Import libraries\n",
    "import import_ipynb\n",
    "from Input_Tools import *\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import os as os\n",
    "from textblob import TextBlob\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as mdates\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define API Key\n",
    "Define the API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_Eodhd = \"66c0aeb1357b15.87356825\"\n",
    "\n",
    "#Important link: https://eodhd.com/financial-academy/financial-faq/fundamentals-glossary-common-stock\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running correlation between all the stocks in the exchange\n",
    "Creates a list of all the correlations betweens stocks and sorts them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_sort_correlations(tickers_df, exchange, instrument_type=\"Common Stock\", api_token=API_Eodhd, start_date=\"2025-01-01\", end_date=\"2025-03-30\", period=\"d\"):\n",
    "    \"\"\"\n",
    "    Calculate the correlation between all stocks in an exchange and sort them from least to highest.\n",
    "\n",
    "    Parameters:\n",
    "        tickers_df (pd.DataFrame): DataFrame containing stock tickers.\n",
    "        exchange (str): Exchange code to append to tickers (e.g., \"US\").\n",
    "        instrument_type (str): Type of the instrument to filter (e.g., \"Common Stock\").\n",
    "        api_token (str): API token for authentication.\n",
    "        start_date (str): Start date for historical data.\n",
    "        end_date (str): End date for historical data.\n",
    "        period (str): Period for historical data (e.g., \"d\" for daily).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing stock pairs and their correlation, sorted by correlation value.\n",
    "    \"\"\"\n",
    "    # Filter tickers to include only the specified instrument type\n",
    "    filtered_tickers_df = tickers_df[tickers_df[\"Type\"] == instrument_type]\n",
    "\n",
    "    stock_data = {}\n",
    "    for ticker in filtered_tickers_df['Code']:\n",
    "        try:\n",
    "            # Combine ticker with exchange code\n",
    "            full_ticker = f\"{ticker}.{exchange}\"\n",
    "            stock_df = Fetch_historical_price(full_ticker, start_date, end_date, period, api_token, columns=[\"adjusted_close\"])\n",
    "            stock_data[ticker] = stock_df.set_index(\"date\")[\"adjusted_close\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for {ticker}: {e}\")\n",
    "\n",
    "    # Combine all stock data into a single DataFrame\n",
    "    combined_df = pd.DataFrame(stock_data)\n",
    "\n",
    "    # Calculate correlation matrix\n",
    "    correlation_matrix = combined_df.corr()\n",
    "\n",
    "    # Create a list of all pairs and their correlations\n",
    "    correlations = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i + 1, len(correlation_matrix.columns)):\n",
    "            stock1 = correlation_matrix.columns[i]\n",
    "            stock2 = correlation_matrix.columns[j]\n",
    "            correlation = correlation_matrix.iloc[i, j]\n",
    "            correlations.append({\"Instrument 1\": stock1, \"Instrument 2\": stock2, \"Correlation\": correlation})\n",
    "\n",
    "    # Convert the list of correlations to a DataFrame\n",
    "    correlations_df = pd.DataFrame(correlations)\n",
    "\n",
    "    # Sort the DataFrame by correlation value\n",
    "    correlations_df = correlations_df.sort_values(by=\"Correlation\", ascending=True).reset_index(drop=True)\n",
    "\n",
    "    return correlations_df\n",
    "\n",
    "# Example usage\n",
    "tickers = fetch_exchange_symbols(\"AS\")\n",
    "tickers_subset = tickers\n",
    "sorted_correlations_df = calculate_and_sort_correlations(tickers_subset, \"AS\", instrument_type=\"ETF\")\n",
    "print(sorted_correlations_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimized version\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def fetch_stock_data(ticker, exchange, start_date, end_date, period, api_token):\n",
    "    \"\"\"Helper function to fetch stock data for a single ticker.\"\"\"\n",
    "    try:\n",
    "        full_ticker = f\"{ticker}.{exchange}\"\n",
    "        stock_df = Fetch_historical_price(full_ticker, start_date, end_date, period, api_token, columns=[\"adjusted_close\"])\n",
    "        return ticker, stock_df.set_index(\"date\")[\"adjusted_close\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        return ticker, None\n",
    "\n",
    "def calculate_and_sort_correlations(tickers_df, exchange, instrument_type=\"Common Stock\", api_token=API_Eodhd, start_date=\"2025-01-01\", end_date=\"2025-03-30\", period=\"d\"):\n",
    "    \"\"\"\n",
    "    Calculate the correlation between all stocks in an exchange and sort them from least to highest.\n",
    "\n",
    "    Parameters:\n",
    "        tickers_df (pd.DataFrame): DataFrame containing stock tickers.\n",
    "        exchange (str): Exchange code to append to tickers (e.g., \"US\").\n",
    "        instrument_type (str): Type of the instrument to filter (e.g., \"Common Stock\").\n",
    "        api_token (str): API token for authentication.\n",
    "        start_date (str): Start date for historical data.\n",
    "        end_date (str): End date for historical data.\n",
    "        period (str): Period for historical data (e.g., \"d\" for daily).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing stock pairs and their correlation, sorted by correlation value.\n",
    "    \"\"\"\n",
    "    # Filter tickers to include only the specified instrument type\n",
    "    filtered_tickers_df = tickers_df[tickers_df[\"Type\"] == instrument_type]\n",
    "\n",
    "    # Fetch stock data in parallel\n",
    "    stock_data = {}\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [\n",
    "            executor.submit(fetch_stock_data, ticker, exchange, start_date, end_date, period, api_token)\n",
    "            for ticker in filtered_tickers_df['Code']\n",
    "        ]\n",
    "        for future in futures:\n",
    "            ticker, data = future.result()\n",
    "            if data is not None:\n",
    "                stock_data[ticker] = data\n",
    "\n",
    "    # Combine all stock data into a single DataFrame\n",
    "    combined_df = pd.DataFrame(stock_data).dropna(axis=1, how=\"any\")  # Drop columns with NaN values\n",
    "\n",
    "    # Calculate correlation matrix\n",
    "    correlation_matrix = combined_df.corr()\n",
    "\n",
    "    # Flatten the correlation matrix into a DataFrame\n",
    "    correlations_df = (\n",
    "        correlation_matrix.stack()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"level_0\": \"Instrument 1\", \"level_1\": \"Instrument 2\", 0: \"Correlation\"})\n",
    "    )\n",
    "\n",
    "    # Remove self-correlations and duplicates\n",
    "    correlations_df = correlations_df[correlations_df[\"Instrument 1\"] < correlations_df[\"Instrument 2\"]]\n",
    "\n",
    "    # Sort the DataFrame by correlation value\n",
    "    correlations_df = correlations_df.sort_values(by=\"Correlation\", ascending=True).reset_index(drop=True)\n",
    "\n",
    "    return correlations_df\n",
    "\n",
    "# Example usage\n",
    "tickers = fetch_exchange_symbols(\"AS\")\n",
    "tickers_subset = tickers\n",
    "sorted_correlations_df = calculate_and_sort_correlations(tickers_subset, \"AS\", instrument_type=\"ETF\")\n",
    "print(sorted_correlations_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting output to excel\n",
    "Creates an excel export in the python folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output directory and file path\n",
    "output_dir = r\"E:\\Business_NL\\Python\\Excel output\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # Ensure the directory exists\n",
    "output_file = os.path.join(output_dir, \"sorted_correlations.xlsx\")\n",
    "\n",
    "# Export the DataFrame to an Excel file\n",
    "sorted_correlations_df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Sorted correlations exported to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing sentement with finbert model\n",
    "Creates a function to use finbert model to analyze sentement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_sentiment_with_finbert(news_df):\n",
    "    \"\"\"\n",
    "    Analyze the sentiment of news articles using FinBERT.\n",
    "\n",
    "    Parameters:\n",
    "        news_df (pd.DataFrame): DataFrame containing news articles.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with an additional 'Sentiment' column.\n",
    "    \"\"\"\n",
    "    if 'content' not in news_df.columns:\n",
    "        raise ValueError(\"The DataFrame must contain a 'content' column for sentiment analysis.\")\n",
    "    \n",
    "    # Load the FinBERT sentiment analysis pipeline with explicit truncation\n",
    "    finbert = pipeline(\n",
    "        \"sentiment-analysis\",\n",
    "        model=\"yiyanghkust/finbert-tone\",\n",
    "        tokenizer=\"yiyanghkust/finbert-tone\",\n",
    "        device=0,  # Use CPU (-1) or GPU (0 or higher)\n",
    "        truncation=True,\n",
    "        max_length=512  # Explicitly set the maximum length\n",
    "    )\n",
    "    \n",
    "    # Apply sentiment analysis to the 'content' column\n",
    "    def analyze_text(text):\n",
    "        try:\n",
    "            return finbert(text[:512])[0]['label']  # Truncate text to 512 characters\n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\"\n",
    "    \n",
    "    news_df['Sentiment'] = news_df['content'].apply(analyze_text)\n",
    "    \n",
    "    return news_df\n",
    "\n",
    "# Example usage\n",
    "api_token = API_Eodhd  # Replace with your actual API token\n",
    "stock = \"AAPL.US\"\n",
    "begin_date = \"2023-01-01\"\n",
    "end_date = \"2025-03-01\"\n",
    "tag = \"balance sheet\"\n",
    "\n",
    "# Fetch news data\n",
    "news_df = fetch_news_data(stock, tag, begin_date, end_date, api_token=api_token)\n",
    "\n",
    "# Perform sentiment analysis with FinBERT\n",
    "news_with_sentiment = analyze_sentiment_with_finbert(news_df)\n",
    "\n",
    "print(news_with_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trading strategy: Use last 12 months dividends to price a stock/option\n",
    "A trading strategy where I use last 12 months dividends and price the stock/option at 6% dividend yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fetch_dividend_share(tickers_df, exchange):\n",
    "    \"\"\"\n",
    "    Fetch dividend share for all stocks in the tickers_df and combine them into a single DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        tickers_df (list or pd.Series): List of stock tickers (e.g., [\"INGA\", \"ASML\"]).\n",
    "        exchange (str): Exchange code (e.g., \"AS\").\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with two columns: 'Ticker' and 'DividendYield(%)'.\n",
    "    \"\"\"\n",
    "    combined_data = []  # Initialize an empty list to store data\n",
    "    full_tickers = [f\"{ticker}.{exchange}\" for ticker in tickers_df]\n",
    "\n",
    "    for stock in full_tickers:\n",
    "        try:\n",
    "            # Fetch the dividend share for the current stock\n",
    "            url = f'https://eodhd.com/api/fundamentals/{stock}?api_token={API_Eodhd}&filter=Highlights::DividendShare&fmt=json'\n",
    "            dividend_share = requests.get(url).json()\n",
    "\n",
    "                   \n",
    "            # Append the stock and dividend yield to the list\n",
    "            combined_data.append({\"Ticker\": stock, \"Dividendshare\": dividend_share})\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for {stock}: {e}\")\n",
    "\n",
    "    # Convert the combined data into a DataFrame\n",
    "    combined_df = pd.DataFrame(combined_data)\n",
    "\n",
    "    # Drop rows with missing dividend yield values\n",
    "    combined_df = combined_df.dropna(subset=[\"Dividendshare\"])\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "def Fetch_dividend_yield(tickers_df):\n",
    "    \"\"\"\n",
    "    Fetch dividend share for all stocks in the tickers_df and combine them into a single DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        tickers_df (list or pd.Series): List of stock tickers (e.g., [\"INGA.AS\", \"ASML.AS\"]).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with two columns: 'Ticker' and 'Dividendshare'.\n",
    "    \"\"\"\n",
    "    combined_data = []  # Initialize an empty list to store data\n",
    "\n",
    "    for stock in tickers_df:\n",
    "        try:\n",
    "            # Fetch the dividend share for the current stock\n",
    "            url = f'https://eodhd.com/api/fundamentals/{stock}?api_token={API_Eodhd}&filter=Highlights::DividendYield&fmt=json'\n",
    "            dividend_yield = requests.get(url).json()\n",
    "\n",
    "            # Append the stock and dividend yield to the list\n",
    "            combined_data.append({\"Ticker\": stock, \"Dividend_yield\": dividend_yield*100})\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for {stock}: {e}\")\n",
    "\n",
    "    # Convert the combined data into a DataFrame\n",
    "    combined_df = pd.DataFrame(combined_data)\n",
    "\n",
    "    # Drop rows with missing dividend yield values\n",
    "    combined_df = combined_df.dropna(subset=[\"Dividend_yield\"])\n",
    "\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a theoretical value of the stock\n",
    "tickers = [\"INGA\", \"ASML\", \"ABN\"]\n",
    "Exchange = \"AS\"\n",
    "Dividend_share = Fetch_dividend_share(tickers, Exchange).dropna(subset=[\"Dividendshare\"])\n",
    "\n",
    "Theoretical_DY = 0.06\n",
    "\n",
    "# Ensure 'Dividendshare' is numeric, replace non-numeric values with 0\n",
    "Dividend_share[\"Dividendshare\"] = pd.to_numeric(Dividend_share[\"Dividendshare\"], errors='coerce').fillna(0)\n",
    "\n",
    "# Calculate the theoretical stock price, handling division by zero\n",
    "Dividend_share[\"TheoreticalStockPrice\"] = np.where(\n",
    "    Theoretical_DY != 0,  # Check if the denominator is not zero\n",
    "    Dividend_share[\"Dividendshare\"] / Theoretical_DY,  # Perform the division\n",
    "    0  # Return 0 if the denominator is zero\n",
    ")\n",
    "\n",
    "# Initialize a dictionary to store the 'High' prices for each ticker\n",
    "high_prices = {}\n",
    "\n",
    "# Loop through each ticker to fetch the latest price data and extract the 'High' value\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        # Fetch the latest price data\n",
    "        latest_price = fetch_intraday_data(f\"{ticker}.{Exchange}\").iloc[0]\n",
    "        \n",
    "        # Extract the 'High' value and store it in the dictionary\n",
    "        high_prices[f\"{ticker}.{Exchange}\"] = latest_price[\"high\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        high_prices[f\"{ticker}.{Exchange}\"] = None\n",
    "\n",
    "# Add the 'High' values to the Dividend_share DataFrame\n",
    "Dividend_share[\"StockPrice\"] = Dividend_share[\"Ticker\"].map(high_prices)\n",
    "\n",
    "# Calculating upside/downside potential, handling division by zero\n",
    "Dividend_share[\"Potential\"] = np.where(\n",
    "    Dividend_share[\"TheoreticalStockPrice\"] != 0,  # Check if the denominator is not zero\n",
    "    (Dividend_share[\"TheoreticalStockPrice\"] - Dividend_share[\"StockPrice\"]) * 100 / Dividend_share[\"TheoreticalStockPrice\"],  # Perform the calculation\n",
    "    0  # Return 0 if the denominator is zero\n",
    ")\n",
    "\n",
    "# Round the specified columns to 2 decimal places\n",
    "Dividend_share[\"TheoreticalStockPrice\"] = Dividend_share[\"TheoreticalStockPrice\"].round(2)\n",
    "Dividend_share[\"StockPrice\"] = Dividend_share[\"StockPrice\"].round(2)\n",
    "Dividend_share[\"Potential\"] = Dividend_share[\"Potential\"].round(2)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(Dividend_share)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the best stocks according to this model\n",
    "#small caps\n",
    "Small_Caps_list = [\"PBH\", \"PORF\", \"LVIDE\", \"VALUE\", \"NSE\", \"CTAC\", \"BEVER\", \"NEDSE\", \"ECT\"]\n",
    "Exchange = \"AS\"\n",
    "Dividend_share = Fetch_dividend_share(Small_Caps_list, Exchange).dropna(subset=[\"Dividendshare\"])\n",
    "\n",
    "Theoretical_DY = 0.06\n",
    "\n",
    "# Ensure 'Dividendshare' is numeric, replace non-numeric values with 0\n",
    "Dividend_share[\"Dividendshare\"] = pd.to_numeric(Dividend_share[\"Dividendshare\"], errors='coerce').fillna(0)\n",
    "\n",
    "# Calculate the theoretical stock price, handling division by zero\n",
    "Dividend_share[\"TheoreticalStockPrice\"] = np.where(\n",
    "    Theoretical_DY != 0,  # Check if the denominator is not zero\n",
    "    Dividend_share[\"Dividendshare\"] / Theoretical_DY,  # Perform the division\n",
    "    0  # Return 0 if the denominator is zero\n",
    ")\n",
    "\n",
    "# Initialize a dictionary to store the 'High' prices for each ticker\n",
    "high_prices = {}\n",
    "\n",
    "# Loop through each ticker to fetch the latest price data and extract the 'High' value\n",
    "for ticker in Small_Caps_list:\n",
    "    try:\n",
    "        # Fetch the latest price data\n",
    "        latest_price = fetch_intraday_data(f\"{ticker}.{Exchange}\").iloc[0]\n",
    "        \n",
    "        # Extract the 'High' value and store it in the dictionary\n",
    "        high_prices[f\"{ticker}.{Exchange}\"] = latest_price[\"high\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        high_prices[f\"{ticker}.{Exchange}\"] = None\n",
    "\n",
    "# Add the 'High' values to the Dividend_share DataFrame\n",
    "Dividend_share[\"StockPrice\"] = Dividend_share[\"Ticker\"].map(high_prices)\n",
    "\n",
    "# Calculating upside/downside potential, handling division by zero\n",
    "Dividend_share[\"Potential\"] = np.where(\n",
    "    Dividend_share[\"TheoreticalStockPrice\"] != 0,  # Check if the denominator is not zero\n",
    "    (Dividend_share[\"TheoreticalStockPrice\"] - Dividend_share[\"StockPrice\"]) * 100 / Dividend_share[\"TheoreticalStockPrice\"],  # Perform the calculation\n",
    "    0  # Return 0 if the denominator is zero\n",
    ")\n",
    "\n",
    "# Round the specified columns to 2 decimal places\n",
    "Dividend_share[\"TheoreticalStockPrice\"] = Dividend_share[\"TheoreticalStockPrice\"].round(2)\n",
    "Dividend_share[\"StockPrice\"] = Dividend_share[\"StockPrice\"].round(2)\n",
    "Dividend_share[\"Potential\"] = Dividend_share[\"Potential\"].round(2)\n",
    "\n",
    "# Define the output directory and file path\n",
    "output_dir = r\"E:\\Business NL\\Python\\Excel output\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # Ensure the directory exists\n",
    "output_file = os.path.join(output_dir, \"sorted_correlations.xlsx\")\n",
    "\n",
    "# Export the DataFrame to an Excel file\n",
    "Dividend_share.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Sorted correlations exported to {output_file}\")\n",
    "\n",
    "#Mid-caps\n",
    "Mid_Caps_List = [\"HYDRA\", \"KENDR\", \"SLIGR\", \"AJAX\", \"NEDAP\", \"PHARM\",\"ACOMO\", \"TOM2\",\"AMG\", \"PNL\",\"AXS\",\"BRNL\",\"WHA\",\"FFARM\",\"NSI\",\"SIFG\",\"BSGR\",\"ALFEN\",\n",
    "                 \"ENVI\",\"FAST\",\"CMCOM\",\"NXFIL\",\"AZRN\",\"TLT\"]\n",
    "\n",
    "Dividend_share = Fetch_dividend_share(Mid_Caps_List, Exchange).dropna(subset=[\"Dividendshare\"])\n",
    "\n",
    "Theoretical_DY = 0.06\n",
    "\n",
    "# Ensure 'Dividendshare' is numeric, replace non-numeric values with 0\n",
    "Dividend_share[\"Dividendshare\"] = pd.to_numeric(Dividend_share[\"Dividendshare\"], errors='coerce').fillna(0)\n",
    "\n",
    "# Calculate the theoretical stock price, handling division by zero\n",
    "Dividend_share[\"TheoreticalStockPrice\"] = np.where(\n",
    "    Theoretical_DY != 0,  # Check if the denominator is not zero\n",
    "    Dividend_share[\"Dividendshare\"] / Theoretical_DY,  # Perform the division\n",
    "    0  # Return 0 if the denominator is zero\n",
    ")\n",
    "\n",
    "# Initialize a dictionary to store the 'High' prices for each ticker\n",
    "high_prices = {}\n",
    "\n",
    "# Loop through each ticker to fetch the latest price data and extract the 'High' value\n",
    "for ticker in Mid_Caps_List:\n",
    "    try:\n",
    "        # Fetch the latest price data\n",
    "        latest_price = fetch_intraday_data(f\"{ticker}.{Exchange}\").iloc[0]\n",
    "        \n",
    "        # Extract the 'High' value and store it in the dictionary\n",
    "        high_prices[f\"{ticker}.{Exchange}\"] = latest_price[\"high\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        high_prices[f\"{ticker}.{Exchange}\"] = None\n",
    "\n",
    "# Add the 'High' values to the Dividend_share DataFrame\n",
    "Dividend_share[\"StockPrice\"] = Dividend_share[\"Ticker\"].map(high_prices)\n",
    "\n",
    "# Calculating upside/downside potential, handling division by zero\n",
    "Dividend_share[\"Potential\"] = np.where(\n",
    "    Dividend_share[\"TheoreticalStockPrice\"] != 0,  # Check if the denominator is not zero\n",
    "    (Dividend_share[\"TheoreticalStockPrice\"] - Dividend_share[\"StockPrice\"]) * 100 / Dividend_share[\"TheoreticalStockPrice\"],  # Perform the calculation\n",
    "    0  # Return 0 if the denominator is zero\n",
    ")\n",
    "\n",
    "# Round the specified columns to 2 decimal places\n",
    "Dividend_share[\"TheoreticalStockPrice\"] = Dividend_share[\"TheoreticalStockPrice\"].round(2)\n",
    "Dividend_share[\"StockPrice\"] = Dividend_share[\"StockPrice\"].round(2)\n",
    "Dividend_share[\"Potential\"] = Dividend_share[\"Potential\"].round(2)\n",
    "\n",
    "print(Dividend_share)\n",
    "\n",
    "# Large caps\n",
    "Large_Caps_List = [\"KPN\",\"MT\",\"OCI\",\"WKL\",\"APAM\",\"BAMNB\",\"PHIA\",\"RAND\",\"CRBN\",\"HEIO\",\"ASM\",\"AKZA\",\"REN\",\"AALB\",\"HAL\",\"HEIA\",\"AGN\",\"ASML\",\"SBMO\",\"BESI\",\n",
    "                   \"SHELL\",\"VPK\",\"TFG\",\"TWEKA\",\"FUR\",\"UNA\",\"INGA\",\"AD\",\"ABN\",\"ARCAD\",\"ASRNL\",\"BFIT\",\"ECMPA\",\"FLOW\",\"GLPG\",\"HEIJM\",\"IMCD\",\"NRP\",\"NN\",\"PSH\",\n",
    "                   \"LIGHT\",\"TKWY\",\"VLK\",\"ADYEN\",\"PRX\",\"CCEP\",\"REINA\",\"JDEP\",\"INPST\",\"CTPNV\",\"ALLFG\",\"UMG\",\"EXO\",\"DSFIR\",\"FER\",\"THEON\",\"CVC\"]\n",
    "\n",
    "Dividend_share = Fetch_dividend_share(Mid_Caps_List, Exchange).dropna(subset=[\"Dividendshare\"])\n",
    "\n",
    "Theoretical_DY = 0.06\n",
    "\n",
    "# Ensure 'Dividendshare' is numeric, replace non-numeric values with 0\n",
    "Dividend_share[\"Dividendshare\"] = pd.to_numeric(Dividend_share[\"Dividendshare\"], errors='coerce').fillna(0)\n",
    "\n",
    "# Calculate the theoretical stock price, handling division by zero\n",
    "Dividend_share[\"TheoreticalStockPrice\"] = np.where(\n",
    "    Theoretical_DY != 0,  # Check if the denominator is not zero\n",
    "    Dividend_share[\"Dividendshare\"] / Theoretical_DY,  # Perform the division\n",
    "    0  # Return 0 if the denominator is zero\n",
    ")\n",
    "\n",
    "# Initialize a dictionary to store the 'High' prices for each ticker\n",
    "high_prices = {}\n",
    "\n",
    "# Loop through each ticker to fetch the latest price data and extract the 'High' value\n",
    "for ticker in Mid_Caps_List:\n",
    "    try:\n",
    "        # Fetch the latest price data\n",
    "        latest_price = fetch_intraday_data(f\"{ticker}.{Exchange}\").iloc[0]\n",
    "        \n",
    "        # Extract the 'High' value and store it in the dictionary\n",
    "        high_prices[f\"{ticker}.{Exchange}\"] = latest_price[\"high\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        high_prices[f\"{ticker}.{Exchange}\"] = None\n",
    "\n",
    "# Add the 'High' values to the Dividend_share DataFrame\n",
    "Dividend_share[\"StockPrice\"] = Dividend_share[\"Ticker\"].map(high_prices)\n",
    "\n",
    "# Calculating upside/downside potential, handling division by zero\n",
    "Dividend_share[\"Potential\"] = np.where(\n",
    "    Dividend_share[\"TheoreticalStockPrice\"] != 0,  # Check if the denominator is not zero\n",
    "    (Dividend_share[\"TheoreticalStockPrice\"] - Dividend_share[\"StockPrice\"]) * 100 / Dividend_share[\"TheoreticalStockPrice\"],  # Perform the calculation\n",
    "    0  # Return 0 if the denominator is zero\n",
    ")\n",
    "\n",
    "# Round the specified columns to 2 decimal places\n",
    "Dividend_share[\"TheoreticalStockPrice\"] = Dividend_share[\"TheoreticalStockPrice\"].round(2)\n",
    "Dividend_share[\"StockPrice\"] = Dividend_share[\"StockPrice\"].round(2)\n",
    "Dividend_share[\"Potential\"] = Dividend_share[\"Potential\"].round(2)\n",
    "\n",
    "print(Dividend_share)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output directory and file path\n",
    "output_dir = r\"E:\\Business_NL\\Python\\Excel output\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # Ensure the directory exists\n",
    "output_file = os.path.join(output_dir, \"dividend_shares.xlsx\")\n",
    "\n",
    "# Create a dictionary to store DataFrames for each category\n",
    "dividend_data = {}\n",
    "\n",
    "# Small Caps\n",
    "Small_Caps_list = [\"PBH\", \"PORF\", \"LVIDE\", \"VALUE\", \"NSE\", \"CTAC\", \"BEVER\", \"NEDSE\", \"ECT\"]\n",
    "Dividend_share_small = Fetch_dividend_share(Small_Caps_list, Exchange).dropna(subset=[\"Dividendshare\"])\n",
    "Dividend_share_small[\"Dividendshare\"] = pd.to_numeric(Dividend_share_small[\"Dividendshare\"], errors='coerce').fillna(0)\n",
    "Dividend_share_small[\"TheoreticalStockPrice\"] = np.where(\n",
    "    Theoretical_DY != 0,\n",
    "    Dividend_share_small[\"Dividendshare\"] / Theoretical_DY,\n",
    "    0\n",
    ")\n",
    "\n",
    "# Calculate StockPrice and Potential for Small Caps\n",
    "high_prices = {}\n",
    "for ticker in Small_Caps_list:\n",
    "    try:\n",
    "        latest_price = fetch_intraday_data(f\"{ticker}.{Exchange}\").iloc[0]\n",
    "        high_prices[f\"{ticker}.{Exchange}\"] = latest_price[\"high\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        high_prices[f\"{ticker}.{Exchange}\"] = None\n",
    "\n",
    "Dividend_share_small[\"StockPrice\"] = Dividend_share_small[\"Ticker\"].map(high_prices)\n",
    "Dividend_share_small[\"Potential\"] = np.where(\n",
    "    Dividend_share_small[\"TheoreticalStockPrice\"] != 0,\n",
    "    (Dividend_share_small[\"TheoreticalStockPrice\"] - Dividend_share_small[\"StockPrice\"]) * 100 / Dividend_share_small[\"TheoreticalStockPrice\"],\n",
    "    0\n",
    ")\n",
    "Dividend_share_small = Dividend_share_small.round(2)\n",
    "dividend_data[\"Small Caps\"] = Dividend_share_small\n",
    "\n",
    "# Mid Caps\n",
    "Mid_Caps_List = [\"HYDRA\", \"KENDR\", \"SLIGR\", \"AJAX\", \"NEDAP\", \"PHARM\", \"ACOMO\", \"TOM2\", \"AMG\", \"PNL\", \"AXS\", \"BRNL\", \"WHA\", \"FFARM\", \"NSI\", \"SIFG\", \"BSGR\", \"ALFEN\", \"ENVI\", \"FAST\", \"CMCOM\", \"NXFIL\", \"AZRN\", \"TLT\"]\n",
    "Dividend_share_mid = Fetch_dividend_share(Mid_Caps_List, Exchange).dropna(subset=[\"Dividendshare\"])\n",
    "Dividend_share_mid[\"Dividendshare\"] = pd.to_numeric(Dividend_share_mid[\"Dividendshare\"], errors='coerce').fillna(0)\n",
    "Dividend_share_mid[\"TheoreticalStockPrice\"] = np.where(\n",
    "    Theoretical_DY != 0,\n",
    "    Dividend_share_mid[\"Dividendshare\"] / Theoretical_DY,\n",
    "    0\n",
    ")\n",
    "\n",
    "# Calculate StockPrice and Potential for Mid Caps\n",
    "high_prices = {}\n",
    "for ticker in Mid_Caps_List:\n",
    "    try:\n",
    "        latest_price = fetch_intraday_data(f\"{ticker}.{Exchange}\").iloc[0]\n",
    "        high_prices[f\"{ticker}.{Exchange}\"] = latest_price[\"high\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        high_prices[f\"{ticker}.{Exchange}\"] = None\n",
    "\n",
    "Dividend_share_mid[\"StockPrice\"] = Dividend_share_mid[\"Ticker\"].map(high_prices)\n",
    "Dividend_share_mid[\"Potential\"] = np.where(\n",
    "    Dividend_share_mid[\"TheoreticalStockPrice\"] != 0,\n",
    "    (Dividend_share_mid[\"TheoreticalStockPrice\"] - Dividend_share_mid[\"StockPrice\"]) * 100 / Dividend_share_mid[\"TheoreticalStockPrice\"],\n",
    "    0\n",
    ")\n",
    "Dividend_share_mid = Dividend_share_mid.round(2)\n",
    "dividend_data[\"Mid Caps\"] = Dividend_share_mid\n",
    "\n",
    "# Large Caps\n",
    "Large_Caps_List = [\"KPN\", \"MT\", \"OCI\", \"WKL\", \"APAM\", \"BAMNB\", \"PHIA\", \"RAND\", \"CRBN\", \"HEIO\", \"ASM\", \"AKZA\", \"REN\", \"AALB\", \"HAL\", \"HEIA\", \"AGN\", \"ASML\", \"SBMO\", \"BESI\", \"SHELL\", \"VPK\", \"TFG\", \"TWEKA\", \"FUR\", \"UNA\", \"INGA\", \"AD\", \"ABN\", \"ARCAD\", \"ASRNL\", \"BFIT\", \"ECMPA\", \"FLOW\", \"GLPG\", \"HEIJM\", \"IMCD\", \"NRP\", \"NN\", \"PSH\", \"LIGHT\", \"TKWY\", \"VLK\", \"ADYEN\", \"PRX\", \"CCEP\", \"REINA\", \"JDEP\", \"INPST\", \"CTPNV\", \"ALLFG\", \"UMG\", \"EXO\", \"DSFIR\", \"FER\", \"THEON\", \"CVC\"]\n",
    "Dividend_share_large = Fetch_dividend_share(Large_Caps_List, Exchange).dropna(subset=[\"Dividendshare\"])\n",
    "Dividend_share_large[\"Dividendshare\"] = pd.to_numeric(Dividend_share_large[\"Dividendshare\"], errors='coerce').fillna(0)\n",
    "Dividend_share_large[\"TheoreticalStockPrice\"] = np.where(\n",
    "    Theoretical_DY != 0,\n",
    "    Dividend_share_large[\"Dividendshare\"] / Theoretical_DY,\n",
    "    0\n",
    ")\n",
    "\n",
    "# Calculate StockPrice and Potential for Large Caps\n",
    "high_prices = {}\n",
    "for ticker in Large_Caps_List:\n",
    "    try:\n",
    "        latest_price = fetch_intraday_data(f\"{ticker}.{Exchange}\").iloc[0]\n",
    "        high_prices[f\"{ticker}.{Exchange}\"] = latest_price[\"high\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        high_prices[f\"{ticker}.{Exchange}\"] = None\n",
    "\n",
    "Dividend_share_large[\"StockPrice\"] = Dividend_share_large[\"Ticker\"].map(high_prices)\n",
    "Dividend_share_large[\"Potential\"] = np.where(\n",
    "    Dividend_share_large[\"TheoreticalStockPrice\"] != 0,\n",
    "    (Dividend_share_large[\"TheoreticalStockPrice\"] - Dividend_share_large[\"StockPrice\"]) * 100 / Dividend_share_large[\"TheoreticalStockPrice\"],\n",
    "    0\n",
    ")\n",
    "Dividend_share_large = Dividend_share_large.round(2)\n",
    "dividend_data[\"Large Caps\"] = Dividend_share_large\n",
    "\n",
    "# Export all DataFrames to a single Excel file with separate sheets\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    for sheet_name, df in dividend_data.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(f\"Dividend shares exported to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separating the stocks into 3 quintiles\n",
    "Separating the stocks into small cap, midcap and large cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the daily trading market volume = close price * volume\n",
    "# Take an average of the last 30 days\n",
    "# separate them into three quintiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting rolling correlation between two stocks\n",
    "Understand the rolling correlation between two stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates \n",
    "# Calculate the start date for the last 3 years\n",
    "end_date = dt.datetime.now()\n",
    "start_date = end_date - dt.timedelta(days=3 * 365)\n",
    "\n",
    "\n",
    "# Define stock details\n",
    "stock1 = \"INGA\"\n",
    "exchange1 = \"AS\"\n",
    "stock2 = \"ABN\"\n",
    "exchange2 = \"AS\"\n",
    "period = \"d\"\n",
    "\n",
    "# Function to fetch historical prices and plot rolling correlation\n",
    "def plot_rolling_correlation(stock1, exchange1, stock2, exchange2, start_date, end_date, window=90):\n",
    "    # Fetch historical prices for both stocks\n",
    "    stock1_df = Fetch_historical_price(stock1, exchange1, start_date, end_date, period, API_Eodhd)\n",
    "    stock2_df = Fetch_historical_price(stock2, exchange2, start_date, end_date, period, API_Eodhd)\n",
    "\n",
    "    # Ensure both DataFrames have a 'date' column and convert it to datetime\n",
    "    stock1_df['date'] = pd.to_datetime(stock1_df['date'])\n",
    "    stock2_df['date'] = pd.to_datetime(stock2_df['date'])\n",
    "\n",
    "    # Sort the DataFrames by date and set the 'date' column as the index\n",
    "    stock1_df = stock1_df.sort_values(by='date').set_index('date')\n",
    "    stock2_df = stock2_df.sort_values(by='date').set_index('date')\n",
    "\n",
    "    # Align the two DataFrames on their dates\n",
    "    combined_df = pd.concat([stock1_df['adjusted_close'], stock2_df['adjusted_close']], axis=1, keys=['Stock1', 'Stock2']).dropna()\n",
    "    \n",
    "    # Calculate daily returns for both stocks\n",
    "    combined_df['Stock1_Returns'] = combined_df['Stock1'].pct_change()\n",
    "    combined_df['Stock2_Returns'] = combined_df['Stock2'].pct_change()\n",
    "\n",
    "    # Calculate rolling correlation\n",
    "    rolling_corr = combined_df['Stock1'].rolling(window=window).corr(combined_df['Stock2'])\n",
    "\n",
    "    # Plot the rolling correlation with color based on value\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i in range(1, len(rolling_corr)):\n",
    "        if rolling_corr[i] > 0:\n",
    "            plt.plot(rolling_corr.index[i-1:i+1], rolling_corr[i-1:i+1], color='red')\n",
    "        else:\n",
    "            plt.plot(rolling_corr.index[i-1:i+1], rolling_corr[i-1:i+1], color='green')\n",
    "\n",
    "    plt.title(f'{window}-Day Rolling Correlation Between {stock1} and {stock2}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Correlation')\n",
    "    plt.grid()\n",
    "\n",
    "    # Format the date axis\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=3))  # Show every 12 months\n",
    "    plt.gcf().autofmt_xdate()  # Rotate date labels for better readability\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to plot the rolling correlation\n",
    "plot_rolling_correlation(stock1, exchange1, stock2, exchange2, start_date, end_date, window=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting multiple rolling correlations\n",
    "\n",
    "plot_rolling_correlation(\"INGA\", \"AS\", \"AAPL\", \"US\", start_date, end_date, window=90)\n",
    "plot_rolling_correlation(\"INGA\", \"AS\", \"IUSA\", \"AS\", start_date, end_date, window=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a 4 factor model\n",
    "Factor 1 - Correlation with portfolio\n",
    "Factor 2 - Dividend yield\n",
    "Factor 3 - Price momentum (55 days)\n",
    "Factor 4 - Price momentum (25 days)\n",
    "Equal weighting to all factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating my portfolio\n",
    "\n",
    "# Define the path to the Excel file\n",
    "\n",
    "excel_file_path_portfolio = r\"E:\\Business_NL\\Python\\Portfolio_list.xlsx\"\n",
    "\n",
    "# Read column C from the Excel file into a pandas DataFrame\n",
    "# Assuming column C contains the data you need and has a header\n",
    "portfolio_list_stock = pd.read_excel(excel_file_path_portfolio, usecols=\"C\")\n",
    "portfolio_list_quantity = pd.read_excel(excel_file_path_portfolio, usecols=\"D\")\n",
    "\n",
    "# Define the start and end dates\n",
    "end_date = dt.datetime.now()\n",
    "start_date = end_date - dt.timedelta(weeks=52*7)\n",
    "\n",
    "# Getting portfolio information\n",
    "portfolio_stocks = portfolio_list_stock[\"Stock_name\"]\n",
    "portfolio_quantity = portfolio_list_quantity[\"Quantity\"]\n",
    "Margin_amount = 16000\n",
    "\n",
    "# Initializing the portfolio DataFrame\n",
    "portfolio_init = pd.DataFrame({\n",
    "    \"Stock\": portfolio_stocks,\n",
    "    \"Quantity\": portfolio_quantity\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "# Initialize a DataFrame to store daily portfolio values\n",
    "daily_portfolio_values = pd.DataFrame()\n",
    "\n",
    "# Loop through each ticker in the portfolio\n",
    "for index, row in portfolio_init.iterrows():\n",
    "    ticker = row[\"Stock\"]\n",
    "    quantity = row[\"Quantity\"]\n",
    "    try:\n",
    "        # Fetch historical price data (adjusted close prices)\n",
    "        stock_data = Fetch_historical_price(\n",
    "            f\"{ticker}\", start_date, end_date, period=\"d\", columns=[\"adjusted_close\"]\n",
    "        )\n",
    "        \n",
    "        # Add a column for the stock's daily value (price * quantity)\n",
    "        stock_data[\"Daily_Value\"] = stock_data[\"adjusted_close\"] * quantity\n",
    "        \n",
    "        # Add the stock's daily value to the portfolio DataFrame\n",
    "        if daily_portfolio_values.empty:\n",
    "            daily_portfolio_values = stock_data[[\"date\", \"Daily_Value\"]].rename(columns={\"Daily_Value\": ticker})\n",
    "        else:\n",
    "            daily_portfolio_values = daily_portfolio_values.merge(\n",
    "                stock_data[[\"date\", \"Daily_Value\"]].rename(columns={\"Daily_Value\": ticker}),\n",
    "                on=\"date\",\n",
    "                how=\"outer\"\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {ticker}: {e}\")\n",
    "\n",
    "# Calculate the total portfolio value for each day\n",
    "daily_portfolio_values[\"Total_Portfolio_Value\"] = daily_portfolio_values.drop(columns=[\"date\"]).sum(axis=1) - Margin_amount\n",
    "\n",
    "# Add the margin amount as a column\n",
    "daily_portfolio_values[\"Margin_Amount\"] = Margin_amount\n",
    "\n",
    "# Drop rows with NaN values\n",
    "daily_portfolio_values = daily_portfolio_values.dropna()\n",
    "\n",
    "# Display the daily portfolio values\n",
    "#print(daily_portfolio_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_four_factor_score(stock):\n",
    "    \"\"\"\n",
    "    Calculate the four factors for a stock:\n",
    "    1. Spearman correlation with portfolio daily values\n",
    "    2. Dividend yield\n",
    "    3. Price momentum (55 days)\n",
    "    4. Price momentum (25 days)\n",
    "    Returns: correlation, dividend_yield, momentum_55, momentum_25 (all rounded to 3 decimals)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Calculate log returns for Total_Portfolio_Value\n",
    "        daily_portfolio_values['log_return'] = np.log(\n",
    "            daily_portfolio_values['Total_Portfolio_Value'] / daily_portfolio_values['Total_Portfolio_Value'].shift(1)\n",
    "        )\n",
    "        daily_portfolio_returns = daily_portfolio_values.dropna()\n",
    "        end_date = dt.datetime.now()\n",
    "        start_date = end_date - dt.timedelta(days=90)\n",
    "\n",
    "        # Factor 1: Spearman correlation with portfolio\n",
    "        def process_stock(stock):\n",
    "            try:\n",
    "                stock_data = Fetch_historical_price(stock, start_date, end_date, period=\"d\", columns=[\"adjusted_close\"])\n",
    "                stock_data['log_return'] = np.log(stock_data['adjusted_close'] / stock_data['adjusted_close'].shift(1))\n",
    "                stock_data = stock_data.dropna()\n",
    "                merged_data = pd.merge(\n",
    "                    daily_portfolio_returns[['date', 'log_return']],\n",
    "                    stock_data[['date', 'log_return']],\n",
    "                    on='date',\n",
    "                    suffixes=('_portfolio', f'_{stock}')\n",
    "                )\n",
    "                correlation = merged_data['log_return_portfolio'].corr(merged_data[f'log_return_{stock}'], method='spearman')\n",
    "                return correlation if correlation is not None else 0\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {stock}: {e}\")\n",
    "                return 0  # If error, treat correlation as 0\n",
    "\n",
    "        correlation = process_stock(stock)\n",
    "\n",
    "        # Factor 2: Dividend yield\n",
    "        dividend_yield = 0\n",
    "        try:\n",
    "            dividend_yield_df = Fetch_dividend_yield([stock])\n",
    "            if not dividend_yield_df.empty and \"Dividend_yield\" in dividend_yield_df.columns:\n",
    "                value = dividend_yield_df.loc[dividend_yield_df['Ticker'] == stock, \"Dividend_yield\"]\n",
    "                if not value.empty and pd.notna(value.values[0]):\n",
    "                    dividend_yield = float(value.values[0])\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching dividend yield for {stock}: {e}\")\n",
    "            dividend_yield = 0  # Explicitly set to zero on any error\n",
    "\n",
    "        # Factor 3: Price momentum (55 days)\n",
    "        try:\n",
    "            momentum_55 = fetch_price_momentum(stock, 55)\n",
    "            if momentum_55 is None or pd.isna(momentum_55):\n",
    "                momentum_55 = 0\n",
    "        except Exception:\n",
    "            momentum_55 = 0\n",
    "\n",
    "        # Factor 4: Price momentum (25 days)\n",
    "        try:\n",
    "            momentum_25 = fetch_price_momentum(stock, 25)\n",
    "            if momentum_25 is None or pd.isna(momentum_25):\n",
    "                momentum_25 = 0\n",
    "        except Exception:\n",
    "            momentum_25 = 0\n",
    "\n",
    "        # Round all outputs to 3 decimals\n",
    "        return (\n",
    "            round(correlation, 3),\n",
    "            round(dividend_yield, 3),\n",
    "            round(momentum_55, 3),\n",
    "            round(momentum_25, 3)\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating four-factor values for {stock}: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "# Example usage\n",
    "stock = \"ABN.AS\"\n",
    "start_date = \"2023-01-01\"\n",
    "end_date = \"2023-12-31\"\n",
    "\n",
    "correlation, dividend_yield, momentum_55, momentum_25 = calculate_four_factor_score(stock)\n",
    "print(f\"  Factor 1 (Correlation): {correlation}\")\n",
    "print(f\"  Factor 2 (Dividend Yield): {dividend_yield}\")\n",
    "print(f\"  Factor 3 (Momentum 55): {momentum_55}\")\n",
    "print(f\"  Factor 4 (Momentum 25): {momentum_25}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of exchanges\n",
    "exchange_list = [\"AS\",\"PA\",\"US\"]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "combined_data = []\n",
    "\n",
    "# Loop through each exchange and fetch the data\n",
    "for exchange in exchange_list:\n",
    "    df = fetch_stocks_by_market_cap_and_exchange(100000000000, exchange)\n",
    "    combined_data.append(df)\n",
    "\n",
    "# Combine all DataFrames into a single DataFrame\n",
    "stock_list = pd.concat(combined_data, ignore_index=True)\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(stock_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define start and end dates for the analysis\n",
    "end_date = dt.datetime.now()\n",
    "start_date = end_date - dt.timedelta(days=90)\n",
    "\n",
    "# Function to wrap the four-factor calculation for parallel execution\n",
    "def run_four_factor(stock):\n",
    "    # Returns: (correlation, dividend_yield, momentum_55, momentum_25)\n",
    "    return (stock, *calculate_four_factor_score(stock))\n",
    "\n",
    "# Run in parallel for all stocks in stock_list[\"Stock_name\"]\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    results = list(executor.map(run_four_factor, stock_list[\"Stock_name\"]))\n",
    "\n",
    "# Convert results to DataFrame with factor titles (no Raw_Score column)\n",
    "four_factor_df = pd.DataFrame(\n",
    "    results,\n",
    "    columns=[\n",
    "        \"Stock\",\n",
    "        \"Correlation_with_Portfolio\",\n",
    "        \"Dividend_Yield\",\n",
    "        \"Momentum_55d\",\n",
    "        \"Momentum_25d\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Drop rows with None values (if any)\n",
    "four_factor_df = four_factor_df.dropna(subset=[\"Correlation_with_Portfolio\", \"Dividend_Yield\", \"Momentum_55d\", \"Momentum_25d\"])\n",
    "\n",
    "# Normalize each factor to the range [-1, 1]\n",
    "factor_columns = [\"Correlation_with_Portfolio\", \"Dividend_Yield\", \"Momentum_55d\", \"Momentum_25d\"]\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "four_factor_df[[f + \"_Norm\" for f in factor_columns]] = scaler.fit_transform(four_factor_df[factor_columns])\n",
    "\n",
    "# Specify coefficients for each factor\n",
    "correlation_coef = -0.25\n",
    "dividend_yield_coef = 0.25\n",
    "momentum_55_coef = 0.25\n",
    "momentum_25_coef = 0.25\n",
    "\n",
    "# Calculate normalized score using the specified coefficients\n",
    "four_factor_df[\"Normalized_Score\"] = (\n",
    "    correlation_coef * four_factor_df[\"Correlation_with_Portfolio_Norm\"] +\n",
    "    dividend_yield_coef * four_factor_df[\"Dividend_Yield_Norm\"] +\n",
    "    momentum_55_coef * four_factor_df[\"Momentum_55d_Norm\"] +\n",
    "    momentum_25_coef * four_factor_df[\"Momentum_25d_Norm\"]\n",
    ").round(3)\n",
    "\n",
    "# Sort by highest normalized score\n",
    "four_factor_df = four_factor_df.sort_values(by=\"Normalized_Score\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(four_factor_df[[\n",
    "    \"Stock\",\n",
    "    \"Normalized_Score\",\n",
    "    \"Correlation_with_Portfolio_Norm\",\n",
    "    \"Dividend_Yield_Norm\",\n",
    "    \"Momentum_55d_Norm\",\n",
    "    \"Momentum_25d_Norm\"\n",
    "]].head(10))  # Display top 10 stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output four_factor_df to Excel\n",
    "output_dir = r\"E:\\Business_NL\\Python\\Excel output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, \"four_factor_scores.xlsx\")\n",
    "\n",
    "four_factor_df.to_excel(output_file, index=False)\n",
    "print(f\"Four-factor scores exported to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a logistic regression model\n",
    "The model will try to predict if the value after a month will be higher or lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date  adjusted_close_ABN  adjusted_close_KBW  Total_Portfolio_Value  \\\n",
      "0 2018-06-19           -0.000449            0.000540              -0.013941   \n",
      "1 2018-06-20            0.012123           -0.000900               0.016043   \n",
      "2 2018-06-21           -0.014839            0.000360              -0.024951   \n",
      "3 2018-06-22            0.025045           -0.010132              -0.001301   \n",
      "4 2018-06-25           -0.015125           -0.011704              -0.095628   \n",
      "\n",
      "   volume_ABN  volume_KBW  \n",
      "0     2897670      114661  \n",
      "1     2244582      139888  \n",
      "2     2336461      236573  \n",
      "3     1995351      855619  \n",
      "4     1101921      215020  \n"
     ]
    }
   ],
   "source": [
    "# Define the start and end dates\n",
    "end_date = dt.datetime.now()\n",
    "start_date = end_date - dt.timedelta(weeks=52*7)\n",
    "# Retriving stock price\n",
    "ABN_price = Fetch_historical_price(\"ABN.AS\", start_date, end_date, period=\"d\", columns=[\"adjusted_close\",\"volume\"])\n",
    "KBW_price = Fetch_historical_price(\"KBWB.US\", start_date, end_date, period=\"d\", columns=[\"adjusted_close\",\"volume\"])\n",
    "daily_portfolio_values = daily_portfolio_values.dropna()\n",
    "\n",
    "\n",
    "# Merging data into a single Dataframe\n",
    "\n",
    "# Ensure 'date' columns are datetime for proper merging\n",
    "ABN_price['date'] = pd.to_datetime(ABN_price['date'])\n",
    "KBW_price['date'] = pd.to_datetime(KBW_price['date'])\n",
    "daily_portfolio_values['date'] = pd.to_datetime(daily_portfolio_values['date'])\n",
    "\n",
    "# Merge ABN_price and KBW_price on 'date'\n",
    "merged_df = pd.merge(ABN_price, KBW_price, on='date', suffixes=('_ABN', '_KBW'))\n",
    "\n",
    "\n",
    "# Merge only the 'Total_Portfolio_Value' column from daily_portfolio_values\n",
    "Combined_df = pd.merge(\n",
    "    merged_df,\n",
    "    daily_portfolio_values[['date', 'Total_Portfolio_Value']],\n",
    "    on='date',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Columns for log returns\n",
    "log_return_cols = ['adjusted_close_ABN', 'adjusted_close_KBW', 'Total_Portfolio_Value']\n",
    "\n",
    "# Columns to keep as absolute values (exclude date and log return columns)\n",
    "abs_cols = [col for col in Combined_df.columns if col not in log_return_cols + ['date']]\n",
    "\n",
    "# Calculate log returns for selected columns\n",
    "log_returns = np.log(Combined_df[log_return_cols] / Combined_df[log_return_cols].shift(1))\n",
    "\n",
    "# Get absolute values for other columns (shift to align with log returns)\n",
    "abs_values = Combined_df[abs_cols].iloc[1:].reset_index(drop=True)\n",
    "\n",
    "# Align log_returns index with abs_values\n",
    "log_returns = log_returns.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "# Add the date column back (skip the first row to match log_returns)\n",
    "date_col = Combined_df['date'].iloc[1:].reset_index(drop=True)\n",
    "\n",
    "# Combine all into a single DataFrame\n",
    "Combined_df_log = pd.concat([date_col, log_returns, abs_values], axis=1)\n",
    "\n",
    "# Reorder columns to make 'date' the first column\n",
    "cols = Combined_df_log.columns.tolist()\n",
    "cols = ['date'] + [col for col in cols if col != 'date']\n",
    "Combined_df_log = Combined_df_log[cols]\n",
    "\n",
    "print(Combined_df_log.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          date  \\\n",
      "0    2025-05-22T14:09:23+00:00   \n",
      "1    2025-05-15T07:04:26+00:00   \n",
      "2    2025-05-15T05:31:45+00:00   \n",
      "3    2025-05-14T05:14:00+00:00   \n",
      "4    2025-05-09T13:07:23+00:00   \n",
      "..                         ...   \n",
      "300  2020-08-12T07:05:45+00:00   \n",
      "301  2020-08-12T05:00:00+00:00   \n",
      "302  2020-07-24T11:00:45+00:00   \n",
      "303  2020-07-24T11:00:45+00:00   \n",
      "304  2020-06-17T06:00:01+00:00   \n",
      "\n",
      "                                               content  \n",
      "0    The Dutch government has lowered its stake in ...  \n",
      "1    Net Profit: EUR619 million. Return on Equity: ...  \n",
      "2    As European markets navigate a landscape marke...  \n",
      "3    ABN AMRO\\n\\nABN AMRO Bank posts net profit of ...  \n",
      "4    The latest developments around tariffs remain ...  \n",
      "..                                                 ...  \n",
      "300  By Bart H. Meijer\\n\\nAMSTERDAM (Reuters) - ABN...  \n",
      "301  ABN AMRO reports net loss of EUR 5 million for...  \n",
      "302  AMSTERDAM (Reuters) - Dutch bank ABN Amro on F...  \n",
      "303  AMSTERDAM (Reuters) - Dutch bank ABN Amro on F...  \n",
      "304  By Cheng Leng, Engen Tham and Tom Daly\\n\\nBEIJ...  \n",
      "\n",
      "[305 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "end_date = dt.datetime.now()\n",
    "start_date = end_date - dt.timedelta(weeks=52*5)\n",
    "start_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "end_str = end_date.strftime(\"%Y-%m-%d\")\n",
    "ABN_news = Fetch_news_data(\"ABN.AS\", start_str, end_str, offset=0, api_token=API_Eodhd)\n",
    "# Filter only 'date' and 'content' columns\n",
    "ABN_news = ABN_news[['date', 'content']]\n",
    "\n",
    "print(ABN_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date  polarity\n",
      "0  2025-05-22     0.977\n",
      "1  2025-05-15     0.997\n",
      "2  2025-05-15     0.997\n",
      "3  2025-05-14     1.000\n",
      "4  2025-05-09     1.000\n"
     ]
    }
   ],
   "source": [
    "# ...existing code...\n",
    "end_date = dt.datetime.now()\n",
    "start_date = end_date - dt.timedelta(weeks=52*5)\n",
    "start_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "end_str = end_date.strftime(\"%Y-%m-%d\")\n",
    "ABN_news = Fetch_news_data(\"ABN.AS\", start_str, end_str, offset=0, api_token=API_Eodhd)\n",
    "# Filter only 'date' and 'content' columns\n",
    "ABN_news = ABN_news[['date','sentiment']]\n",
    "\n",
    "# Convert 'date' to just date (YYYY-MM-DD)\n",
    "ABN_news['date'] = pd.to_datetime(ABN_news['date']).dt.date\n",
    "\n",
    "# Extract 'polarity' from the 'sentiment' dictionary in ABN_news\n",
    "\n",
    "# If 'sentiment' is a string representation of a dict, use ast.literal_eval to convert\n",
    "import ast\n",
    "\n",
    "def extract_polarity(sentiment):\n",
    "    if isinstance(sentiment, dict):\n",
    "        return sentiment.get('polarity', None)\n",
    "    try:\n",
    "        sentiment_dict = ast.literal_eval(sentiment)\n",
    "        return sentiment_dict.get('polarity', None)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "ABN_news['polarity'] = ABN_news['sentiment'].apply(extract_polarity)\n",
    "\n",
    "print(ABN_news[['date', 'polarity']].head())\n",
    "\n",
    "\n",
    "# Group by date and take the mean polarity for each date (handles multiple news per day)\n",
    "polarity_by_date = (\n",
    "    ABN_news.groupby('date')['polarity']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "         date  avg_finbert_score\n",
      "0  2020-06-17           0.999940\n",
      "1  2020-07-24           0.998417\n",
      "2  2020-08-12           0.999953\n",
      "3  2020-08-24           0.999963\n",
      "4  2020-09-16           0.988015\n",
      "5  2020-11-11           0.999999\n",
      "6  2020-11-17           0.999109\n",
      "7  2020-11-30           0.999961\n",
      "8  2021-02-10           0.999589\n",
      "9  2021-03-02           0.999975\n"
     ]
    }
   ],
   "source": [
    "\n",
    "end_date = dt.datetime.now()\n",
    "start_date = end_date - dt.timedelta(weeks=52*5)\n",
    "start_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "end_str = end_date.strftime(\"%Y-%m-%d\")\n",
    "ABN_news = Fetch_news_data(\"ABN.AS\", start_str, end_str, offset=0, api_token=API_Eodhd)\n",
    "# Filter only 'date' and 'content' columns\n",
    "ABN_news = ABN_news[['date', 'content']]\n",
    "\n",
    "# Convert 'date' to just date (YYYY-MM-DD)\n",
    "ABN_news['date'] = pd.to_datetime(ABN_news['date']).dt.date\n",
    "\n",
    "from transformers import pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Use FinBERT to analyze sentiment for each news content\n",
    "finbert = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"yiyanghkust/finbert-tone\",\n",
    "    tokenizer=\"yiyanghkust/finbert-tone\",\n",
    "    device=-1,  # Use -1 for CPU\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "def get_finbert_score(text):\n",
    "    try:\n",
    "        output = finbert(str(text)[:512])\n",
    "        return output[0]['score']\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Ensure 'date' is datetime for grouping, and remove time part\n",
    "ABN_news[\"date\"] = pd.to_datetime(ABN_news[\"date\"]).dt.date\n",
    "\n",
    "# Apply FinBERT sentiment scoring (actual score, not mapped)\n",
    "ABN_news[\"finbert_score\"] = ABN_news[\"content\"].apply(get_finbert_score)\n",
    "\n",
    "# Group by date and calculate average FinBERT sentiment score for each day\n",
    "daily_sentiment = (\n",
    "    ABN_news.groupby(\"date\")[\"finbert_score\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"finbert_score\": \"avg_finbert_score\"})\n",
    ")\n",
    "\n",
    "print(\"Device set to use cpu\")\n",
    "print(daily_sentiment.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date  adjusted_close_ABN  volume_ABN  adjusted_close_KBW  volume_KBW  \\\n",
      "0  2020-06-16            0.069609    0.137662            0.022273    0.107321   \n",
      "1  2020-06-17           -0.016265    0.054480           -0.023741   -0.514224   \n",
      "2  2020-06-18           -0.016788   -0.321381            0.000000   -0.045133   \n",
      "3  2020-06-19           -0.023012    0.510204           -0.006396    0.328715   \n",
      "4  2020-06-22           -0.008578   -1.036782           -0.006940   -0.467136   \n",
      "\n",
      "   adjusted_close_JPM  volume_JPM  Total_Portfolio_Value  avg_finbert_score  \n",
      "0            0.007968    0.001804               0.098782            0.00000  \n",
      "1           -0.025604   -0.349985               0.009881            0.99994  \n",
      "2           -0.005443    0.059096              -0.010762            0.00000  \n",
      "3           -0.011486    0.889202               0.022180            0.00000  \n",
      "4           -0.010897   -0.912063              -0.034371            0.00000  \n"
     ]
    }
   ],
   "source": [
    "# Merge daily_sentiment with Combined_df_log on 'date', fill missing avg_finbert_score with 0\n",
    "# Ensure both 'date' columns are datetime.date type for proper merging\n",
    "Combined_df_log['date'] = pd.to_datetime(Combined_df_log['date']).dt.date\n",
    "daily_sentiment['date'] = pd.to_datetime(daily_sentiment['date']).dt.date\n",
    "\n",
    "# Merge on 'date'\n",
    "Combined_df_log_with_sentiment = pd.merge(\n",
    "    Combined_df_log,\n",
    "    daily_sentiment,\n",
    "    on='date',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing avg_finbert_score with 0\n",
    "Combined_df_log_with_sentiment['avg_finbert_score'] = Combined_df_log_with_sentiment['avg_finbert_score'].fillna(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date  adjusted_close_ABN  adjusted_close_KBW  Total_Portfolio_Value  \\\n",
      "0 2024-06-20            0.006464            0.000757               0.008653   \n",
      "1 2024-06-21           -0.004522           -0.005695              -0.004168   \n",
      "2 2024-06-24            0.009031            0.016328              -0.000115   \n",
      "3 2024-06-25           -0.010973           -0.011581              -0.002785   \n",
      "4 2024-06-26           -0.007818           -0.003635               0.001586   \n",
      "\n",
      "   volume_ABN  volume_KBW  polarity  \n",
      "0     1900763      918926  0.000000  \n",
      "1     4111381      668936  0.968500  \n",
      "2     2232462     1325620  0.498500  \n",
      "3     1487734      341121  0.999000  \n",
      "4     1997544      447869  0.332333  \n"
     ]
    }
   ],
   "source": [
    "#Splitting a combined DataFrame into training and test sets based on date\n",
    "\n",
    "# Ensure date columns are datetime.date for merging\n",
    "Combined_df_log['date'] = pd.to_datetime(Combined_df_log['date']).dt.date\n",
    "# Ensure polarity_by_date is defined before this cell!\n",
    "# Remove any existing 'polarity', 'polarity_x', or 'polarity_y' columns to avoid merge errors\n",
    "for col in ['polarity', 'polarity_x', 'polarity_y']:\n",
    "    if col in Combined_df_log.columns:\n",
    "        Combined_df_log = Combined_df_log.drop(columns=[col])\n",
    "\n",
    "polarity_by_date['date'] = pd.to_datetime(polarity_by_date['date']).dt.date\n",
    "\n",
    "# Merge with Combined_df_log\n",
    "Combined_df_log = pd.merge(\n",
    "    Combined_df_log,\n",
    "    polarity_by_date,\n",
    "    on='date',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing polarity values with 0\n",
    "Combined_df_log['polarity'] = Combined_df_log['polarity'].fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "# Ensure 'date' column is datetime\n",
    "Combined_df_log['date'] = pd.to_datetime(Combined_df_log['date'])\n",
    "\n",
    "# Sort by date just in case\n",
    "Combined_df_log = Combined_df_log.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# Get the minimum date\n",
    "min_date = Combined_df_log['date'].min()\n",
    "\n",
    "# Define training and test period\n",
    "train_end_date = min_date + pd.DateOffset(years=6)\n",
    "test_end_date = train_end_date + pd.DateOffset(years=2)\n",
    "\n",
    "# Split the data\n",
    "train_df = Combined_df_log[(Combined_df_log['date'] >= min_date) & (Combined_df_log['date'] < train_end_date)].reset_index(drop=True)\n",
    "test_df = Combined_df_log[(Combined_df_log['date'] >= train_end_date) & (Combined_df_log['date'] < test_end_date)].reset_index(drop=True)\n",
    "\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date  adjusted_close_ABN  adjusted_close_KBW  \\\n",
      "1492 2024-06-12            0.014042            0.013485   \n",
      "1493 2024-06-13           -0.020485           -0.005372   \n",
      "1494 2024-06-14           -0.014008           -0.005596   \n",
      "1495 2024-06-17            0.004580            0.011542   \n",
      "1496 2024-06-18            0.006187            0.009326   \n",
      "\n",
      "      Total_Portfolio_Value  volume_ABN  volume_KBW  polarity  target_1m  \n",
      "1492               0.005851     2517999      517204     0.998          0  \n",
      "1493               0.006439     2918350      328389     0.998          0  \n",
      "1494               0.008006     3793421      319084     0.000          0  \n",
      "1495               0.003340     2625417      410230     0.000          0  \n",
      "1496               0.005006     1784772      550588     0.000          0  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "# Prepare the data for logistic regression\n",
    "# We want to predict if adjusted_close_ABN will be positive 1 month (about 21 trading days) from now\n",
    "\n",
    "# Create the target variable: 1 if adjusted_close_ABN is positive 21 days ahead, else 0\n",
    "train_df = train_df.copy()\n",
    "train_df['target_1m'] = (train_df['adjusted_close_ABN'].shift(-21) > 0).astype(int)\n",
    "\n",
    "print (train_df.tail(5))\n",
    "\n",
    "# Drop the last 21 rows where the target is NaN\n",
    "train_df = train_df.dropna(subset=['target_1m'])\n",
    "\n",
    "# Select all predictor columns except 'date', 'target_1m', and the target itself\n",
    "exclude_cols = ['date', 'target_1m']\n",
    "if 'adjusted_close_ABN' in train_df.columns:\n",
    "    exclude_cols.append('adjusted_close_ABN')\n",
    "X = train_df.drop(columns=exclude_cols).values\n",
    "y = train_df['target_1m'].values\n",
    "\n",
    "# Fit logistic regression\n",
    "logreg = LogisticRegression(max_iter=100000)\n",
    "logreg.fit(X, y)\n",
    "\n",
    "# Predict probability of positive value 1 month from now for each row in train_df\n",
    "train_df['prob_positive_1m'] = logreg.predict_proba(X)[:, 1]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.473\n",
      "Test ROC AUC: 0.468\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.50      0.49       120\n",
      "           1       0.47      0.45      0.46       119\n",
      "\n",
      "    accuracy                           0.47       239\n",
      "   macro avg       0.47      0.47      0.47       239\n",
      "weighted avg       0.47      0.47      0.47       239\n",
      "\n",
      "        date  prob_positive_1m  predicted_class  target_1m\n",
      "0 2024-06-20          0.497487                0          1\n",
      "1 2024-06-21          0.507212                1          1\n",
      "2 2024-06-24          0.494686                0          0\n",
      "3 2024-06-25          0.501660                1          0\n",
      "4 2024-06-26          0.502326                1          1\n",
      "\n",
      "Variable importance (absolute value of logistic regression coefficients):\n",
      "volume_KBW              -3.836499e-08\n",
      "volume_ABN               1.325882e-08\n",
      "polarity                -9.194089e-13\n",
      "adjusted_close_KBW       6.383277e-15\n",
      "Total_Portfolio_Value    1.320099e-16\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "\n",
    "# Use the same exclude_cols as in training\n",
    "exclude_cols = ['date', 'target_1m']\n",
    "if 'adjusted_close_ABN' in train_df.columns:\n",
    "    exclude_cols.append('adjusted_close_ABN')\n",
    "\n",
    "# Remove 'prob_positive_1m' from features for both train and test\n",
    "if 'prob_positive_1m' in train_df.columns:\n",
    "    exclude_cols.append('prob_positive_1m')\n",
    "\n",
    "# Prepare test data (use same predictors as training)\n",
    "test_df = test_df.copy()\n",
    "test_df['target_1m'] = (test_df['adjusted_close_ABN'].shift(-21) > 0).astype(int)\n",
    "test_df = test_df.dropna(subset=['target_1m'])\n",
    "\n",
    "# Ensure columns match training data (order and names)\n",
    "feature_cols = [col for col in train_df.columns if col not in exclude_cols]\n",
    "# Only use columns that exist in test_df to avoid KeyError\n",
    "feature_cols_test = [col for col in feature_cols if col in test_df.columns]\n",
    "\n",
    "# Prepare X and y for train and test\n",
    "X_train = train_df[feature_cols].values\n",
    "y_train = train_df['target_1m'].values\n",
    "X_test = test_df[feature_cols_test].values\n",
    "y_test = test_df['target_1m'].values\n",
    "\n",
    "# Retrain logistic regression with correct features\n",
    "logreg = LogisticRegression(max_iter=100000)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities and classes\n",
    "test_df['prob_positive_1m'] = logreg.predict_proba(X_test)[:, 1]\n",
    "test_df['predicted_class'] = (test_df['prob_positive_1m'] >= 0.5).astype(int)\n",
    "\n",
    "# Evaluate model\n",
    "accuracy = accuracy_score(y_test, test_df['predicted_class'])\n",
    "roc_auc = roc_auc_score(y_test, test_df['prob_positive_1m'])\n",
    "report = classification_report(y_test, test_df['predicted_class'])\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Test ROC AUC: {roc_auc:.3f}\")\n",
    "print(\"Classification Report:\\n\", report)\n",
    "print(test_df[['date', 'prob_positive_1m', 'predicted_class', 'target_1m']].head())\n",
    "\n",
    "# Show variable importance (absolute value of coefficients)\n",
    "coefs = logreg.coef_[0]\n",
    "importance = pd.Series(coefs, index=feature_cols).sort_values(key=abs, ascending=False)\n",
    "print(\"\\nVariable importance (absolute value of logistic regression coefficients):\")\n",
    "print(importance)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
